{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "\n",
    "per_author = data.groupby('author')['text'].aggregate(lambda x: ' '.join(list(x)))\n",
    "for doc in per_author:\n",
    "    text = map(lambda x: x.lower(), word_tokenize(doc))\n",
    "    print(FreqDist(text).most_common(100))\n",
    "\n",
    "# stop words and punctuation expectately dominate\n",
    "# Numbers and proper names seem to be rare enough to not pose noteworthy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in per_author:\n",
    "    text = map(lambda x: x.lower(), word_tokenize(doc))\n",
    "    print(FreqDist(filter(lambda x: not x.isascii(), text)).most_common(100))\n",
    "\n",
    "# non ascii elements are used so rarely that we expect them to get eliminated by a high pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def contains_punctuation(s: str) -> bool:\n",
    "    for punct in string.punctuation:\n",
    "        if s.__contains__(punct):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(string.punctuation)\n",
    "for doc in per_author:\n",
    "    text = word_tokenize(doc)\n",
    "    print(FreqDist(filter(contains_punctuation, text)).most_common(100))\n",
    "\n",
    "# All authors do of course use similar punctuation, but there seems enough variation in relative frequency to hypothesize that punctuation constitutes a good stylomatic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in per_author:\n",
    "    text = map(lambda x: x.lower(), word_tokenize(doc))\n",
    "    print(FreqDist(filter(lambda x: x.__contains__(\"'\"), text)).most_common(10))\n",
    "\n",
    "# all authors use contractions to different degrees. Because we are looking for stylomatic featuers we recommend not eliminating contractions.\n",
    "# The noise we see from filtering for the use of an apostrophe will be eliminated by the high pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 17594), ('the', 14969), ('of', 8970), ('.', 7677), ('be', 7250), ('a', 6132), ('and', 5733), ('to', 4761), ('in', 4124), ('i', 3784), ('it', 2933), ('have', 2872), ('that', 2327), ('my', 1788), ('with', 1695), ('``', 1628), ('at', 1588), ('which', 1488), (\"''\", 1359), (';', 1354), ('not', 1347), ('for', 1343), ('he', 1302), ('this', 1296), ('his', 1278), ('by', 1206), ('but', 1200), ('upon', 1025), ('from', 991), ('me', 894), ('all', 864), ('no', 853), ('so', 810), ('an', 806), ('we', 802), ('or', 798), ('you', 768), ('say', 688), ('one', 671), ('there', 612), ('do', 570), ('on', 545), ('very', 537), ('her', 523), ('more', 516), ('?', 510), ('now', 473), ('could', 457), ('make', 449), ('into', 445), ('some', 444), ('than', 443), ('what', 436), ('when', 430), ('him', 420), ('would', 416), ('will', 393), ('about', 393), (\"'s\", 391), ('any', 390), (\"'\", 377), ('these', 358), ('if', 336), ('then', 335), ('they', 330), ('their', 325), ('our', 325), ('who', 321), ('find', 321), ('most', 316), ('time', 314), ('up', 314), ('take', 298), ('she', 295), ('great', 284), ('well', 279), ('little', 275), ('only', 274), ('know', 271), ('eye', 270), ('even', 268), ('out', 266), ('see', 265), ('day', 258), ('man', 256), ('them', 255), ('however', 254), ('thus', 254), ('first', 249), ('here', 247), ('such', 247), ('can', 242), ('long', 241), ('u', 236), ('other', 235), ('yet', 232), ('give', 230), ('much', 230), ('before', 226), ('thing', 221)]\n",
      "[('the', 10933), (',', 8581), ('and', 6098), ('of', 5846), ('.', 5703), ('be', 4428), ('a', 4352), ('to', 3249), ('in', 2737), ('i', 2705), ('have', 2353), ('that', 2021), ('it', 1716), ('he', 1647), ('his', 1171), (';', 1143), ('with', 1122), ('for', 1020), ('but', 979), ('my', 971), ('at', 940), ('on', 933), ('which', 920), ('from', 910), ('not', 894), ('by', 661), ('they', 648), ('an', 645), ('me', 619), (\"'s\", 612), ('all', 549), (\"'\", 549), ('one', 516), ('or', 511), ('when', 503), ('do', 492), ('could', 490), ('this', 489), ('there', 482), ('him', 459), ('thing', 433), ('no', 431), ('old', 418), ('so', 402), ('come', 400), ('we', 370), ('what', 370), ('know', 369), ('some', 368), ('would', 367), ('more', 358), ('out', 325), ('only', 316), ('seem', 316), ('now', 299), ('their', 299), ('``', 297), ('up', 295), ('them', 288), ('into', 284), ('time', 281), ('like', 280), ('see', 279), ('man', 279), ('before', 274), ('who', 273), ('then', 273), ('find', 263), ('say', 263), ('night', 260), ('house', 257), ('very', 249), ('great', 249), ('than', 248), ('after', 247), ('about', 247), ('tell', 241), ('make', 237), ('saw', 236), ('though', 234), ('go', 228), ('where', 224), (\"''\", 216), ('look', 211), ('over', 211), ('through', 211), ('even', 208), ('men', 204), ('down', 202), ('day', 197), ('dream', 195), ('never', 193), ('place', 193), ('hear', 193), ('those', 191), ('think', 189), ('street', 189), ('must', 189), ('if', 187), (\"n't\", 186)]\n",
      "[(',', 12045), ('the', 9657), ('of', 6137), ('and', 6122), ('.', 5711), ('be', 5555), ('to', 4829), ('i', 4314), ('a', 3736), (';', 2662), ('my', 2659), ('in', 2597), ('have', 2134), ('that', 2091), ('her', 1657), ('his', 1646), ('it', 1531), ('with', 1529), ('he', 1484), ('me', 1473), ('not', 1189), ('but', 1172), ('for', 1131), ('on', 1044), ('you', 1044), ('by', 995), ('from', 968), ('which', 961), ('she', 924), ('``', 837), ('this', 828), ('at', 698), ('him', 655), (\"''\", 632), ('we', 610), ('all', 608), ('when', 572), ('do', 556), ('their', 536), ('our', 497), ('one', 489), ('they', 483), ('would', 475), ('will', 455), ('your', 440), ('an', 430), ('or', 423), ('?', 419), ('no', 408), ('now', 398), ('if', 389), ('could', 383), ('so', 364), ('love', 357), (\"'s\", 352), ('life', 349), ('more', 341), (':', 339), ('them', 330), ('who', 326), ('yet', 318), ('day', 288), ('what', 287), ('before', 286), ('eye', 285), ('heart', 284), ('should', 281), ('only', 279), ('time', 277), ('u', 272), ('make', 271), ('these', 271), ('raymond', 270), ('say', 270), ('might', 269), ('even', 264), ('come', 263), ('there', 258), ('some', 251), ('then', 249), ('than', 246), ('find', 242), ('man', 242), ('while', 240), ('become', 239), ('can', 234), ('into', 234), ('take', 234), ('every', 233), ('myself', 217), ('father', 213), ('must', 212), ('own', 211), ('first', 211), ('give', 211), ('friend', 207), ('up', 206), ('hope', 205), ('death', 200), ('upon', 200)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for doc in per_author:\n",
    "    text = list(map(lambda x: x.lower(), word_tokenize(doc)))\n",
    "    new_text = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(text):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        new_text.append(lemma)\n",
    "    print(FreqDist(new_text).most_common(100))\n",
    "\n",
    "# It's non obvious whether lemmatization helps or not. We recommend trying with and without\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 6434), ('and', 2184), ('ing', 1887), ('tha', 1302), ('ent', 1285), ('ere', 1218), ('oft', 1193), ('fth', 1151), ('her', 1151), ('ion', 1147), ('nth', 1065), ('int', 1064), ('hat', 1008), ('tio', 959), ('thi', 913), ('his', 906), (',an', 900), ('ith', 875), ('ter', 835), ('ver', 827), ('for', 796), ('hes', 793), ('ati', 788), ('all', 761), ('dth', 737), ('was', 730), ('wit', 729), ('eth', 728), ('nce', 672), ('oth', 653), ('eof', 644), ('tth', 636), ('whi', 626), ('est', 621), ('ear', 621), ('sof', 619), ('eve', 617), ('edt', 600), ('rea', 597), ('hem', 576), ('not', 570), ('ted', 564), ('ess', 560), ('tin', 550), ('con', 542), ('edi', 539), ('ate', 538), ('att', 537), ('hic', 537), ('res', 536), ('ore', 530), ('hea', 516), ('ave', 514), (',th', 514), ('din', 513), ('ght', 510), ('ich', 506), ('ont', 501), ('nde', 494), ('.th', 494), ('ers', 486), ('ast', 482), ('sth', 479), ('tof', 477), ('had', 474), ('rth', 464), ('per', 463), ('ort', 462), ('ble', 457), ('ndt', 454), ('een', 454), ('hen', 451), ('tot', 450), ('hec', 450), ('red', 446), ('ons', 445), ('one', 442), ('igh', 437), ('hin', 435), ('ist', 432), ('men', 430), ('ous', 430), ('hav', 425), ('ein', 423), ('our', 422), ('oul', 420), ('are', 419), ('ain', 417), ('but', 415), ('tho', 415), ('hed', 414), ('eat', 410), ('eri', 410), ('nto', 410), ('ine', 408), ('hou', 405), ('sin', 405), ('oun', 401), ('sto', 399), ('ome', 397)]\n",
      "[('the', 4744), ('and', 2220), ('ing', 1709), ('tha', 1008), ('ere', 987), ('hat', 905), ('her', 830), ('nth', 830), ('ent', 799), (',an', 792), ('dth', 778), ('int', 744), ('was', 740), ('oft', 724), ('ter', 678), ('hes', 663), ('eth', 661), ('fth', 654), ('ght', 640), ('his', 635), ('had', 629), ('for', 625), ('est', 618), ('all', 608), ('thi', 606), ('ion', 588), ('ver', 573), ('ith', 564), ('tth', 556), ('oth', 540), ('rea', 517), ('ear', 513), ('edt', 507), ('sof', 497), ('hen', 492), ('igh', 489), ('wit', 484), ('whi', 476), ('ate', 466), ('ous', 459), ('ess', 457), ('tio', 444), ('ndt', 440), ('ati', 433), ('ast', 432), ('hin', 431), ('not', 427), ('rth', 425), ('ers', 423), ('nce', 421), ('ome', 420), ('one', 419), ('eve', 401), ('hed', 397), ('eda', 387), ('tin', 387), ('att', 385), ('.th', 384), ('ted', 376), ('din', 374), ('ain', 374), ('res', 373), ('sto', 373), ('sth', 372), ('eof', 372), ('hou', 367), ('oun', 362), ('hem', 361), ('out', 361), ('red', 359), ('hea', 354), ('str', 353), ('sin', 353), ('hew', 353), ('ath', 351), ('san', 349), ('ich', 349), ('ble', 348), ('uld', 347), ('dto', 346), ('tho', 344), ('ore', 344), ('ean', 344), ('but', 340), ('edi', 332), ('ran', 332), ('sta', 329), ('oul', 329), ('man', 328), ('ugh', 326), ('rom', 324), ('tof', 324), ('hec', 323), ('ort', 321), ('ont', 317), ('ndi', 315), ('eso', 312), ('nde', 311), ('eto', 310), ('whe', 309)]\n",
      "[('the', 4478), ('and', 2308), ('ing', 1362), ('her', 1279), ('tha', 991), ('ent', 928), ('ere', 910), ('his', 893), ('ion', 880), (',an', 810), ('hat', 805), ('for', 776), ('dth', 751), ('was', 748), ('ear', 705), ('nth', 699), ('edt', 687), ('ith', 663), ('hes', 632), ('ver', 624), ('our', 621), ('ess', 613), ('ter', 607), ('ati', 606), ('tio', 603), ('oft', 598), ('eth', 596), ('wit', 594), ('est', 585), ('fth', 584), ('ght', 574), ('sof', 564), ('oth', 552), ('ted', 550), ('int', 550), ('thi', 550), ('you', 544), ('nce', 530), ('rea', 528), ('eve', 528), ('res', 513), ('she', 497), ('red', 476), ('eof', 474), ('all', 472), ('not', 468), ('had', 460), ('ate', 448), ('uld', 442), ('ast', 436), ('ore', 433), ('hea', 433), ('ave', 429), ('ove', 426), ('whi', 425), ('tth', 422), ('hen', 420), ('ers', 418), ('dto', 413), ('nde', 411), ('ome', 406), ('ndt', 403), ('igh', 402), ('but', 396), ('edi', 393), ('oul', 393), ('rth', 392), ('ath', 387), ('hed', 386), ('ain', 386), ('sth', 385), ('are', 380), ('hem', 379), ('hou', 370), ('con', 369), ('ure', 364), ('wer', 364), ('ont', 361), ('ons', 360), ('tin', 357), ('sto', 356), ('rom', 355), ('ine', 354), ('eco', 349), (',th', 349), ('end', 348), ('ect', 348), ('ill', 348), ('hic', 347), ('eto', 345), ('one', 343), ('tho', 341), ('din', 341), ('oun', 340), ('ean', 339), ('eat', 337), ('ich', 335), ('art', 333), ('nte', 332), ('men', 332)]\n"
     ]
    }
   ],
   "source": [
    "for doc in per_author:\n",
    "    text = ''.join(list(map(lambda x: x.lower(), word_tokenize(doc))))\n",
    "    new_text = []\n",
    "    for i in range(len(text)):\n",
    "        if i % 3 == 0 and i < len(text)-2:\n",
    "            new_text.append(''.join([text[i], text[i+1], text[i+2]]))\n",
    "    print(FreqDist(new_text).most_common(100))\n",
    "\n",
    "# Classical n-grams won't work because they violate the bag of words assumption.\n",
    "# We therefore consider chunks of size 3. We also remove whitespace. There seems to be a fair bit of variations between authors. So might might have to try this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cd29cadc5f7142ed49e76471cda09c680d9497c7d65299e23d82ba42a97c302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
