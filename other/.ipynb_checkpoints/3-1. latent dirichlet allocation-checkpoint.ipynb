{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma, gammaln\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-possibility",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>3-1. Latent Dirichlet Allocation<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data:-Reuters-News-Titles\" data-toc-modified-id=\"Data:-Reuters-News-Titles-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data: Reuters News Titles</a></span></li><li><span><a href=\"#Model:-Basic-LDA\" data-toc-modified-id=\"Model:-Basic-LDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model: Basic LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>M-step</a></span></li><li><span><a href=\"#Variational-lower-bound\" data-toc-modified-id=\"Variational-lower-bound-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Variational lower bound</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></li><li><span><a href=\"#Model:-Smoothed-LDA\" data-toc-modified-id=\"Model:-Smoothed-LDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model: Smoothed LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collapsed-Gibbs-Sampling\" data-toc-modified-id=\"Collapsed-Gibbs-Sampling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collapsed Gibbs Sampling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Run-Gibbs-sampler\" data-toc-modified-id=\"Run-Gibbs-sampler-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Run Gibbs sampler</a></span></li><li><span><a href=\"#Recover-$\\beta$-and-$\\theta$-from-the-sample\" data-toc-modified-id=\"Recover-$\\beta$-and-$\\theta$-from-the-sample-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Recover $\\beta$ and $\\theta$ from the sample</a></span></li></ul></li><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>M-step</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-vertical",
   "metadata": {},
   "source": [
    "## Data: Reuters News Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-hungary",
   "metadata": {},
   "source": [
    "Reuters is a multi-class, multi-label dataset.\n",
    "\n",
    "* 90 classes\n",
    "* 10788 documents\n",
    "    * 7769 training documents\n",
    "    * 3019 testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "* train-test split\n",
    ": The data is already splitted. Just sort it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stops += [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \".\", \"!\", \"?\", \",\", \";\", \":\", \"[\", \"]\", \"{\", \"}\", \"-\", \"+\", \n",
    "    \"_\", \"/\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"<\", \">\", \"|\", \"=\",\n",
    "    \".-\", \".,\", \"'\", '\"', ',\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch titles only\n",
    "# fetch 2000 docs only\n",
    "trainset, testset = [], []\n",
    "vocab = []\n",
    "\n",
    "i = 0\n",
    "for file_id in reuters.fileids():\n",
    "    if file_id.startswith(\"train\"):\n",
    "        doc = [w.lower() for w in reuters.words(file_id) \\\n",
    "                 if (w.isupper()) \\\n",
    "                 if (w.lower() not in stops) \\\n",
    "                 and (not w.isnumeric())]\n",
    "        if doc:\n",
    "            trainset.append(doc)\n",
    "            vocab += doc\n",
    "            i += 1\n",
    "    else:\n",
    "        testset.append([w.lower() for w in reuters.words(file_id) \\\n",
    "                         if (w.isupper()) \\\n",
    "                         and (w.lower() not in stops) \\\n",
    "                         and (not w.isnumeric())])\n",
    "    if i >= 2000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(vocab))\n",
    "word_to_ix = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_ix(seq, vocab=vocab):\n",
    "    # len(vocab), which is the last index, is for the <unk> (unknown) token\n",
    "    unk_idx = len(vocab)\n",
    "    return np.array(list(map(lambda w: word_to_ix.get(w, unk_idx), seq)))\n",
    "\n",
    "data = {\n",
    "    \"train\": list(map(seq_to_ix, trainset)),\n",
    "    \"test\": list(map(seq_to_ix, testset))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-polish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"train\"][0][:5]  # word indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-regard",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model: Basic LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-medication",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-remark",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-breakdown",
   "metadata": {},
   "source": [
    "* $\\alpha, \\beta$: hyperparameters (Dirichlet, Multinomial).\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs, vocab, n_topic, gibbs=False, random_state=0):\n",
    "    if gibbs:\n",
    "        global V, k, N, M, alpha, eta, n_iw, n_di\n",
    "    else:\n",
    "        global V, k, N, M, alpha, beta, gamma, phi\n",
    "        \n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    V = len(vocab)\n",
    "    k = n_topic  # number of topics\n",
    "    N = np.array([doc.shape[0] for doc in docs])\n",
    "    M = len(docs)\n",
    "\n",
    "    print(f\"V: {V}\\nk: {k}\\nN: {N[:10]}...\\nM: {M}\")\n",
    "\n",
    "    # initialize α, β\n",
    "    \n",
    "    if gibbs:\n",
    "        alpha = np.random.gamma(shape=100, scale=0.01, size=1)  # one for all k\n",
    "        eta = np.random.gamma(shape=100, scale=0.01, size=1)  # one for all V\n",
    "        print(f\"α: {alpha}\\nη: {eta}\")\n",
    "        \n",
    "        n_iw = np.zeros((k, V), dtype=int)\n",
    "        n_di = np.zeros((M, k), dtype=int)\n",
    "        print(f\"n_iw: dim {n_iw.shape}\\nn_di: dim {n_di.shape}\")\n",
    "    else:\n",
    "        alpha = np.random.gamma(shape=100, scale=0.01, size=k) #np.random.rand(k)\n",
    "        beta = np.random.dirichlet(np.ones(V), k)\n",
    "        print(f\"α: dim {alpha.shape}\\nβ: dim {beta.shape}\")\n",
    "\n",
    "        # initialize ϕ, γ\n",
    "        ## ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "        gamma = alpha + np.ones((M, k)) * N.reshape(-1, 1) / k\n",
    "\n",
    "        phi = np.ones((M, max(N), k)) / k\n",
    "        for m, N_d in enumerate(N):\n",
    "            phi[m, N_d:, :] = 0  # zero padding for vectorized operations\n",
    "\n",
    "        print(f\"γ: dim {gamma.shape}\\nϕ: dim ({len(phi)}, N_d, {phi[0].shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-message",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-trouble",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k$ be variational parameters for $\\alpha, \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-michigan",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di}^0 := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\beta_{i\\mathbf{w}_{dn}}\\exp\\left(\\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(docs, phi, gamma, alpha, beta):\n",
    "    \"\"\"\n",
    "    Minorize the joint likelihood function via variational inference.\n",
    "    This is the E-step of variational EM algorithm for LDA.\n",
    "    \"\"\"\n",
    "    # optimize phi\n",
    "    for m in range(M):\n",
    "        phi[m, :N[m], :] = (beta[:, docs[m]] * np.exp(psi(gamma[m, :]) - psi(gamma[m, :].sum())).reshape(-1, 1)).T\n",
    "\n",
    "        # Normalize phi\n",
    "        phi[m, :N[m]] /= phi[m, :N[m]].sum(axis=1).reshape(-1, 1)\n",
    "        if np.any(np.isnan(phi)):\n",
    "            raise ValueError(\"phi nan\")\n",
    "\n",
    "    # optimize gamma\n",
    "    gamma = alpha + phi.sum(axis=1)\n",
    "\n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-crystal",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-origin",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-participant",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-firmware",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(docs, phi, gamma, alpha, beta, M):\n",
    "    \"\"\"\n",
    "    maximize the lower bound of the likelihood.\n",
    "    This is the M-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \n",
    "    update of alpha follows from appendix A.2 of Blei et al., 2003.\n",
    "    \"\"\"\n",
    "    # update alpha\n",
    "    alpha = _update(alpha, gamma, M)\n",
    "    \n",
    "    # update beta\n",
    "    for j in range(V):\n",
    "        beta[:, j] = np.array([_phi_dot_w(docs, phi, m, j) for m in range(M)]).sum(axis=0)\n",
    "    beta /= beta.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def _update(var, vi_var, const, max_iter=10000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    From appendix A.2 of Blei et al., 2003.\n",
    "    For hessian with shape `H = diag(h) + 1z1'`\n",
    "    \n",
    "    To update alpha, input var=alpha and vi_var=gamma, const=M.\n",
    "    To update eta, input var=eta and vi_var=lambda, const=k.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iter):\n",
    "        # store old value\n",
    "        var0 = var.copy()\n",
    "        \n",
    "        # g: gradient \n",
    "        psi_sum = psi(vi_var.sum(axis=1)).reshape(-1, 1)\n",
    "        g = const * (psi(var.sum()) - psi(var)) \\\n",
    "            + (psi(vi_var) - psi_sum).sum(axis=0)\n",
    "\n",
    "        # H = diag(h) + 1z1'\n",
    "        z = const * polygamma(1, var.sum())  # z: Hessian constant component\n",
    "        h = -const * polygamma(1, var)       # h: Hessian diagonal component\n",
    "        c = (g / h).sum() / (1./z + (1./h).sum())\n",
    "\n",
    "        # update var\n",
    "        var -= (g - c) / h\n",
    "        \n",
    "        # check convergence\n",
    "        err = np.sqrt(np.mean((var - var0) ** 2))\n",
    "        crit = err < tol\n",
    "        if crit:\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"max_iter={max_iter} reached: values might not be optimal.\")\n",
    "    \n",
    "    #print(err)\n",
    "    return var\n",
    "\n",
    "def _phi_dot_w(docs, phi, d, j):\n",
    "    \"\"\"\n",
    "    \\sum_{n=1}^{N_d} ϕ_{dni} w_{dn}^j\n",
    "    \"\"\"\n",
    "    # doc = np.zeros(docs[m].shape[0] * V, dtype=int)\n",
    "    # doc[np.arange(0, docs[m].shape[0] * V, V) + docs[m]] = 1\n",
    "    # doc = doc.reshape(-1, V)\n",
    "    # lam += phi[m, :N[m], :].T @ doc\n",
    "    return (docs[d] == j) @ phi[m, :N[d], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-symbol",
   "metadata": {},
   "source": [
    "#### Variational lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-authentication",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "L(\\gamma, \\phi; \\alpha, \\beta)\n",
    "  &= \\log\\Gamma(\\sum_{j=1}^k \\alpha_j) - \\sum_{i=1}^k \\log\\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} \\mathbf{w}_{n}^j \\log\\beta_{ij} \\\\\n",
    "  &- \\log\\Gamma(\\sum_{i=1}^k \\gamma_i) + \\sum_{i=1}^k \\log\\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log\\phi_{ni}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(gamma, d, i):\n",
    "    \"\"\"\n",
    "    E[log θ_t] where θ_t ~ Dir(gamma)\n",
    "    \"\"\"\n",
    "    return psi(gamma[d, i]) - psi(np.sum(gamma[d, :]))\n",
    "\n",
    "\n",
    "def dl(lam, i, w_n):\n",
    "    \"\"\"\n",
    "    E[log β_t] where β_t ~ Dir(lam)\n",
    "    \"\"\"\n",
    "    return psi(lam[i, w_n]) - psi(np.sum(lam[i, :]))\n",
    "\n",
    "def vlb(docs, phi, gamma, alpha, beta, M, N, k):\n",
    "    \"\"\"\n",
    "    Average variational lower bound for joint log likelihood.\n",
    "    \"\"\"\n",
    "    lb = 0\n",
    "    for d in range(M):\n",
    "        lb += (\n",
    "            gammaln(np.sum(alpha))\n",
    "            - np.sum(gammaln(alpha))\n",
    "            + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "        )\n",
    "\n",
    "        lb -= (\n",
    "            gammaln(np.sum(gamma[d, :]))\n",
    "            - np.sum(gammaln(gamma[d, :]))\n",
    "            + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "        )\n",
    "\n",
    "        for n in range(N[d]):\n",
    "            w_n = int(docs[d][n])\n",
    "\n",
    "            lb += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "            lb += np.sum([phi[d][n, i] * np.log(beta[i, w_n]) for i in range(k)])\n",
    "            lb -= np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "    return lb / M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-aircraft",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-magazine",
   "metadata": {},
   "source": [
    "* Only on 2,000 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# for beutiful plot later, reorder training set\n",
    "# (don't need to do this)\n",
    "\n",
    "if \"lda_trainset.idx\" in os.listdir():\n",
    "    with open(\"lda_trainset.idx\") as r:\n",
    "        idx = eval(r.read())\n",
    "    docs = np.array(data[\"train\"])[idx].tolist()\n",
    "else:\n",
    "    docs = data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-looking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_lda(docs, vocab, n_topic=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-virgin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N_EPOCH = 1000\n",
    "TOL = 0.1\n",
    "\n",
    "verbose = True\n",
    "lb = -np.inf\n",
    "\n",
    "for epoch in range(N_EPOCH): \n",
    "    # store old value\n",
    "    lb_old = lb\n",
    "    \n",
    "    # Variational EM\n",
    "    phi, gamma = E_step(docs, phi, gamma, alpha, beta)\n",
    "    alpha, beta = M_step(docs, phi, gamma, alpha, beta, M)\n",
    "    \n",
    "    # check anomaly\n",
    "    if np.any(np.isnan(alpha)):\n",
    "        print(\"NaN detected: alpha\")\n",
    "        break\n",
    "    \n",
    "    # check convergence\n",
    "    lb = vlb(docs, phi, gamma, alpha, beta, M, N, k)\n",
    "    err = abs(lb - lb_old)\n",
    "    \n",
    "    # check anomaly\n",
    "    if np.isnan(lb):\n",
    "        print(\"NaN detected: lb\")\n",
    "        break\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"{epoch: 04}:  variational_lb: {lb: .3f},  error: {err: .3f}\")\n",
    "    \n",
    "    if err < TOL:\n",
    "        break\n",
    "else:\n",
    "    warnings.warn(f\"max_iter reached: values might not be optimal.\")\n",
    "\n",
    "print(\" ========== TRAINING FINISHED ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-abuse",
   "metadata": {},
   "source": [
    "* Training result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-allen",
   "metadata": {},
   "source": [
    "1. Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_important(beta_i, n=30):\n",
    "    \"\"\"\n",
    "    find the index of the largest `n` values in a list\n",
    "    \"\"\"\n",
    "    \n",
    "    max_values = beta_i.argsort()[-n:][::-1]\n",
    "    return np.array(vocab)[max_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-cleaning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(f\"TOPIC {i:02}: {n_most_important(beta[i], 9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-flush",
   "metadata": {},
   "source": [
    "2. Topic-word & document-document distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "theta_hat = np.array([np.random.dirichlet(gamma[d], n_sample).mean(0) for d in range(M)])\n",
    "theta_hat /= theta_hat.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(121)\n",
    "n_plot_words = 150\n",
    "sns.heatmap(beta.T[:n_plot_words], xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(f\"Words[:{n_plot_words}]\", fontsize=14)\n",
    "plt.title(\"topic-word distribution\", fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(theta_hat, xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(\"Documents\", fontsize=14)\n",
    "plt.title(\"document-topic distribution\", fontsize=16)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-contents",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-observation",
   "metadata": {},
   "source": [
    "## Model: Smoothed LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-scott",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\beta \\sim \\text{Dir}(\\lambda) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-brake",
   "metadata": {},
   "source": [
    "* $\\alpha, \\eta$: Dirichlet hyperparameters.\n",
    "* $\\beta$: Unsmoothed multinomial hyperparameter.\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-entity",
   "metadata": {},
   "source": [
    "### Collapsed Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-adelaide",
   "metadata": {},
   "source": [
    "$$\n",
    "P(z_{dn}^i=1 | \\mathbf{z}_{(-dn)},\\mathbf{w}) \\propto \\frac{n_{(-dn),iw_{dn}}+\\eta}{n_{(-dn),i\\cdot}+V\\eta} \\frac{n_{(-dn),dj}+\\alpha}{n_{(-dn),d\\cdot}+k\\alpha}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-picture",
   "metadata": {},
   "source": [
    "#### Run Gibbs sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_gibbs(docs, vocab, n_topic, n_gibbs=2000):\n",
    "    \"\"\"\n",
    "    Initialize t=0 state for Gibbs sampling.\n",
    "    Replace initial word-topic assignment ndarray (M, N, N_GIBBS) in-place.\n",
    "    \"\"\"\n",
    "    # initialize variables\n",
    "    init_lda(docs, vocab, n_topic=n_topic, gibbs=True)\n",
    "    \n",
    "    # word-topic assignment\n",
    "    global assign\n",
    "    N_max = max(N)\n",
    "    assign = np.zeros((M, N_max, n_gibbs+1), dtype=int)\n",
    "    print(f\"assign: dim {assign.shape}\")\n",
    "    \n",
    "    # initial assignment\n",
    "    for d in range(M):\n",
    "        for n in range(N[d]):\n",
    "            # randomly assign topic to word w_{dn}\n",
    "            w_dn = docs[d][n]\n",
    "            assign[d, n, 0] = np.random.randint(k)\n",
    "\n",
    "            # increment counters\n",
    "            i = assign[d, n, 0]\n",
    "            n_iw[i, w_dn] += 1\n",
    "            n_di[d, i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conditional_prob(w_dn, d):\n",
    "    \"\"\"\n",
    "    P(z_{dn}^i=1 | z_{(-dn)}, w)\n",
    "    \"\"\"\n",
    "    prob = np.empty(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        # P(w_dn | z_i)\n",
    "        _1 = (n_iw[i, w_dn] + eta) / (n_iw[i, :].sum() + V*eta)\n",
    "        # P(z_i | d)\n",
    "        _2 = (n_di[d, i] + alpha) / (n_di[d, :].sum() + k*alpha)\n",
    "        \n",
    "        prob[i] = _1 * _2\n",
    "    \n",
    "    return prob / prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gibbs(docs, vocab, n_topic, n_gibbs=2000, verbose=True):\n",
    "    \"\"\"\n",
    "    Run collapsed Gibbs sampling\n",
    "    \"\"\"\n",
    "    # initialize required variables\n",
    "    _init_gibbs(docs, vocab, n_topic, n_gibbs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\", \"=\"*10, \"START SAMPLER\", \"=\"*10)\n",
    "    \n",
    "    # run the sampler\n",
    "    for t in range(n_gibbs):\n",
    "        for d in range(M):\n",
    "            for n in range(N[d]):\n",
    "                w_dn = docs[d][n]\n",
    "                \n",
    "                # decrement counters\n",
    "                i_t = assign[d, n, t]  # previous assignment\n",
    "                n_iw[i_t, w_dn] -= 1\n",
    "                n_di[d, i_t] -= 1\n",
    "\n",
    "                # assign new topics\n",
    "                prob = _conditional_prob(w_dn, d)\n",
    "                i_tp1 = np.argmax(np.random.multinomial(1, prob))\n",
    "\n",
    "                # increment counter according to new assignment\n",
    "                n_iw[i_tp1, w_dn] += 1\n",
    "                n_di[d, i_tp1] += 1\n",
    "                assign[d, n, t+1] = i_tp1\n",
    "        \n",
    "        # print out status\n",
    "        if verbose & ((t+1) % 50 == 0):\n",
    "            print(f\"Sampled {t+1}/{n_gibbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-boating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_gibbs(docs, vocab, n_topic=10, n_gibbs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-accordance",
   "metadata": {},
   "source": [
    "#### Recover $\\beta$ and $\\theta$ from the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂ = np.empty((k, V))\n",
    "θ̂ = np.empty((M, k))\n",
    "\n",
    "for j in range(V):\n",
    "    for i in range(k):\n",
    "        β̂[i, j] = (n_iw[i, j] + eta) / (n_iw[i, :].sum() + V*eta)\n",
    "\n",
    "for d in range(M):\n",
    "    for i in range(k):\n",
    "        θ̂[d, i] = (n_di[d, i] + alpha) / (n_di[d, :].sum() + k*alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(121)\n",
    "n_plot_words = 150\n",
    "sns.heatmap(β̂.T[:n_plot_words], xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(f\"Words[:{n_plot_words}]\", fontsize=14)\n",
    "plt.title(\"topic-word distribution\", fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(θ̂, xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(\"Documents\", fontsize=14)\n",
    "plt.title(\"document-topic distribution\", fontsize=16)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-telescope",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-favor",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-thing",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k, \\lambda \\in \\mathbb{R}^{k \\times V}$ be variational parameters for $\\alpha, \\beta, \\eta$.  \n",
    "Suppose further that for $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_i^0 \\sim \\text{Dir}(\\lambda^0)$ where $\\lambda_i^0 = \\eta$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-digit",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di} := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\exp\\left(\\Psi(\\lambda_{iw_{dn}}^t) - \\Psi(\\sum_{j=1}^V \\lambda_{ij}^t) + \\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "            1. for $j=1$ to $V$\n",
    "                1. $\\lambda_{ij} = \\eta + \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-occasion",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-swing",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-flood",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-friday",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedLDA:\n",
    "    \n",
    "    def __init__(self, docs, vocab, k):\n",
    "        self.docs = docs\n",
    "        \n",
    "        self.V = len(vocab)\n",
    "        self.k = k  # number of topics\n",
    "        self.N = np.array([doc.shape[0] for doc in docs])\n",
    "        self.M = len(docs)\n",
    "        \n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        \n",
    "        # initialize model parameters\n",
    "        ##self.beta = np.ones((k, V)) / V\n",
    "        self.alpha = np.random.gamma(100, 0.01, k)\n",
    "        self.eta = np.ones(V)\n",
    "\n",
    "        # initialize variational parameters\n",
    "        # ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "        self.phi = [np.ones((N[d], k)) / k for d in range(M)]\n",
    "        self.gamma = alpha + (N / k).reshape(-1, 1)\n",
    "        self.lam = np.random.gamma(shape=100, scale=0.01, size=(k, V))\n",
    "        \n",
    "        \n",
    "    def _update_phi(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter phi\n",
    "        ϕ_{n, j} ∝ e^[ (Ψ(λ_j) - Ψ(Σλ_j)) + ( Ψ(γ_j) - Ψ(Σγ_j) ) ]\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        k = self.k\n",
    "\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        docs = self.docs\n",
    "\n",
    "        for d in range(M):\n",
    "            for n in range(N[d]):\n",
    "                for i in range(k):\n",
    "                    w_n = int(docs[d][n])\n",
    "                    phi[d][n, i] = np.exp(dl(lam, i, w_n) + dg(gamma, d, i))\n",
    "\n",
    "                # Normalize over topics\n",
    "                phi[d][n, :] = phi[d][n, :] / np.sum(phi[d][n, :])\n",
    "                \n",
    "        return phi\n",
    "    \n",
    "    def _update_gamma(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter gamma\n",
    "        γ_t = α_t + Σ_{n=1}^{N_d} ϕ_{t, n}\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        alpha = self.alpha\n",
    "\n",
    "        gamma = alpha + np.array(\n",
    "            list(map(lambda x: x.sum(axis=0), phi))\n",
    "        )\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def _update_lam(self):\n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        lam = self.lam\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        lam[:] = eta\n",
    "        for d in range(M):  #, desc=\"MINORIZE lam\"):\n",
    "            doc = np.zeros(N[d] * V, dtype=int)\n",
    "            doc[np.arange(0, N[d] * V, V) + docs[d]] = 1\n",
    "            doc = doc.reshape(-1, V)\n",
    "            \n",
    "            lam += phi[d].T @ doc\n",
    "        \n",
    "        return lam\n",
    "        \n",
    "    \n",
    "    def _update_alpha(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update alpha with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = M * (psi(np.sum(alpha)) - psi(alpha)) +\\\n",
    "                (psi(gamma) - psi(np.sum(gamma, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -M * polygamma(1, alpha)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = M * polygamma(1, np.sum(alpha))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            alpha = alpha - (g - c) / h\n",
    "            \n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(alpha - alpha_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_alpha(): max_iter reached.\")\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def _update_eta(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update eta with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        eta = self.eta\n",
    "        lam = self.lam\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            eta_old = eta\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = k * (psi(np.sum(eta)) - psi(eta)) +\\\n",
    "                (psi(lam) - psi(np.sum(lam, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -k * polygamma(1, eta)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = k * polygamma(1, np.sum(eta))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            eta = eta - (g - c) / h\n",
    "\n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(eta - eta_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_eta(): max_iter reached.\")\n",
    "\n",
    "        return eta\n",
    "    \n",
    "    \n",
    "    def _E_step(self):\n",
    "        \"\"\"\n",
    "        E-step of the variational EM algorithm.\n",
    "        Update ϕ, γ, λ.\n",
    "        \"\"\"\n",
    "        self.phi = self._update_phi()\n",
    "        self.gamma = self._update_gamma()\n",
    "        self.lam = self._update_lam()\n",
    "        \n",
    "        \n",
    "    def _M_step(self):\n",
    "        \"\"\"\n",
    "        M-step of the variational EM algorithm.\n",
    "        Update α, η.\n",
    "        \"\"\"\n",
    "        self.alpha = self._update_alpha()\n",
    "        self.eta = self._update_eta()\n",
    "    \n",
    "    \n",
    "    def vlb(self):\n",
    "        \"\"\"\n",
    "        lower bound from variational inference\n",
    "        \"\"\"\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        lam = self.lam\n",
    "        alpha = self.alpha\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        M = self.M\n",
    "        k = self.k\n",
    "        N = self.N\n",
    "        \n",
    "        a0, a1, a2, a3_1, a3_2, a4, a5 = 0., 0., 0., 0., 0., 0., 0.\n",
    "        for d in range(M):\n",
    "            a0 += (\n",
    "                k * (\n",
    "                    gammaln(np.sum(eta)) \n",
    "                    - np.sum(gammaln(eta))\n",
    "                )\n",
    "                + np.sum([(eta[j] - 1) * dl(lam, i, j) for j in range(V) for i in range(k)])\n",
    "            )\n",
    "            a1 += (\n",
    "                gammaln(np.sum(alpha))\n",
    "                - np.sum(gammaln(alpha))\n",
    "                + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "\n",
    "            a4 += (\n",
    "                gammaln(np.sum(gamma[d, :]))\n",
    "                - np.sum(gammaln(gamma[d, :]))\n",
    "                + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "            \n",
    "            for i in range(k):\n",
    "                for j in range(V):\n",
    "                    a3_2 += (\n",
    "                        gammaln(np.sum(lam[i, j]))\n",
    "                        - np.sum(gammaln(lam[i, :]))\n",
    "                        + np.sum((lam[i, j] - 1) * dl(lam, i, j))\n",
    "                    )\n",
    "\n",
    "            for n in range(N[d]):\n",
    "                w_n = int(docs[d][n])\n",
    "                a2 += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "                a3_1 += np.sum([phi[d][n, i] * dl(lam, i, w_n) for i in range(k)])\n",
    "                a5 += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "        return a0 + a1 + a2 + a3_1 - a3_2 - a4 - a5\n",
    "    \n",
    "    \n",
    "    def train(self, max_iter=1000, tol=5, verbose=True):\n",
    "        vlb = -np.inf\n",
    "        \n",
    "        for it in range(max_iter):\n",
    "            old_vlb = vlb\n",
    "            self._E_step()\n",
    "            self._M_step()\n",
    "            \n",
    "            vlb = self.vlb()\n",
    "            err = vlb - old_vlb\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iteration {it+1}: {vlb: .3f} (delta: {err: .2f})\") \n",
    "            \n",
    "            if err < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"max_iter reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-green",
   "metadata": {},
   "source": [
    "```python\n",
    "lda = SmoothedLDA(docs, vocab, k)\n",
    "lda.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-corner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "3-1. Latent Dirichlet Allocation",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
