{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_short_dict = {\"MWS\": \"Mary Shelley\", \"HPL\": \"H. P. Lovecraft\", \"EAP\": \"Edgar Allan Poe\"}\n",
    "df = pd.DataFrame(pd.read_json(\"preprocessing_output/preprocessed_test_W.json\").author)\n",
    "df[\"author_short\"] = df.author\n",
    "df.author = df.author.apply(lambda x: authors_short_dict[x])\n",
    "for filename in os.listdir(\"results_output/\"):\n",
    "    fileparts = filename.split(\"_\")\n",
    "    method = fileparts[1]\n",
    "    if \"template\" in method:\n",
    "        continue\n",
    "    preprocessing = \".\".join(fileparts[2].split(\".\")[:-1])\n",
    "    df[f\"{method} with {preprocessing}\"] = pd.read_json(f\"results_output/results_{method}_{preprocessing}.json\")\n",
    "# df\n",
    "\n",
    "metrics = [(\"Accuracy\", accuracy_score), (\"Precision\", lambda x, y: precision_score(x, y, average=\"macro\")), (\"Recall\",\n",
    "                                                                                                              lambda x, y: recall_score(x, y, average=\"macro\")), (\"F1 Score\", lambda x, y: f1_score(x, y, average=\"macro\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m (name, func) \u001b[39min\u001b[39;00m metrics:\n\u001b[1;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbayes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlda\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m         authors \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: [])\n\u001b[1;32m      5\u001b[0m         \u001b[39mfor\u001b[39;00m author \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mEAP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHPL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMWS\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m             method_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[:, df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?=.*author)|(?=.*\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "total_table = pd.DataFrame()\n",
    "for (name, func) in metrics:\n",
    "    for method in [\"bayes\", \"lda\"]:\n",
    "        authors = defaultdict(lambda: [])\n",
    "        for author in [\"EAP\", \"HPL\", \"MWS\"]:\n",
    "            method_df = df.loc[:, df.columns.str.contains(f\"(?=.*author)|(?=.*{method})\")]\n",
    "            author_df = pd.DataFrame({\"author\": method_df[\"author\"]})\n",
    "            author_df = pd.concat([author_df, method_df.iloc[:, 1:].apply(\n",
    "                    lambda x: [1 if author == i else 0 for i in x])], axis=1) \n",
    "            author_df = author_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "            authors[author] = author_df.iloc[:, 2:].apply(lambda x: func(author_df[\"author_short\"], x))\n",
    "        authors = pd.DataFrame(authors)\n",
    "        total_table = pd.concat([total_table, authors], axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "total_table.columns = pd.MultiIndex.from_product([map(lambda x: x[0], metrics), [\"bayes\", \"lda\"], [\"EAP\", \"HPL\", \"MWS\"]])\n",
    "total_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"bayes\", \"lda\"]:\n",
    "    method_df = df.loc[:, df.columns.str.contains(f\"^(?:(?=.*\\\\b(?:author|{method}|author_short)\\\\b)(?:(?![56]).)*)$\")]\n",
    "    method_df = method_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "    score_df = method_df.iloc[: , 2:].apply(lambda x: [accuracy_score(x, method_df[\"author_short\"]), precision_score(method_df[\"author_short\"], x, average=\"macro\"), recall_score(method_df[\"author_short\"], x, average=\"macro\"), f1_score(method_df[\"author_short\"], x, average=\"macro\")])\n",
    "    score_df[\"metric\"] = [\"accuracy\", \"precision\", \"recall\", \"F1 score\"]\n",
    "    score_df.plot.bar(\"metric\", title=f\"Overview {method}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"bayes\", \"lda\"]:\n",
    "    for (name, func) in metrics:\n",
    "        score_df = {}\n",
    "        method_df = df.loc[:, df.columns.str.contains(\n",
    "            f\"^(?:(?=.*\\\\b(?:author|{method}|author_short)\\\\b)(?:(?![56]).)*)$\")]\n",
    "        for author in [\"EAP\", \"MWS\", \"HPL\"]:\n",
    "            author_df = pd.DataFrame(\n",
    "                {\"author\": method_df[\"author\"]})\n",
    "            author_df = pd.concat([author_df, method_df.iloc[:, 1:].apply(\n",
    "                lambda x: [1 if author == i else 0 for i in x])], axis=1)\n",
    "            author_df = author_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "            score_df[author] = author_df.iloc[:, 2:].apply(\n",
    "                lambda x: func(author_df[\"author_short\"], x))\n",
    "        score_df = pd.DataFrame(score_df)\n",
    "        score_df.plot.bar(title=f\"Author Overview {method} {name}\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, func) in metrics:\n",
    "    preprocess_df = {}\n",
    "    for method in [\"bayes\", \"lda\"]:\n",
    "        score_df = df.loc[:, df.columns.str.contains(f\"^(?:(?=.*\\\\b(?:author|{method}|author_short)\\\\b)(?:(?![56]).)*)$\")]\n",
    "        score_df = score_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "        score_df = score_df.iloc[:, 2:].apply(lambda x: func(score_df[\"author_short\"], x))\n",
    "        preprocess_df[method] = score_df\n",
    "    preprocess_df = pd.DataFrame(preprocess_df)\n",
    "    preprocess_df.plot.bar(title=f\"Method Comparison Overview {name}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, func) in metrics:\n",
    "    method_df = df.loc[:, df.columns.str.contains(f\"^[^56]*$\")]\n",
    "    for preprocessing in range(2,int((len(method_df.columns)) / 2 + 1)):\n",
    "        authors = defaultdict(lambda: [])\n",
    "        preprocess_name = \"\"\n",
    "        for author in [\"EAP\", \"HPL\", \"MWS\"]:\n",
    "            author_df = pd.DataFrame(\n",
    "                            {\"author\": method_df[\"author\"]})\n",
    "            author_df = pd.concat([author_df, method_df.iloc[:, 1:].apply(\n",
    "                lambda x: [1 if author == i else 0 for i in x])], axis=1)\n",
    "            for method in [\"bayes\", \"lda\"]:\n",
    "                process_df = author_df.loc[:, author_df.columns.str.contains(f\"(.*{method}.*)|(.*author.*)\", regex=True)]\n",
    "                preprocess_name = re.sub(\".*\\s(\\S+)$\", r\"\\1\", process_df.columns[preprocessing])\n",
    "                authors[author].append(func(process_df[\"author_short\"],process_df.iloc[:, preprocessing]))\n",
    "        authors = pd.DataFrame(authors)\n",
    "        authors = authors.rename(columns=lambda x: authors_short_dict[x])\n",
    "        authors[\"method\"] = [\"bayes\", \"lda\"]\n",
    "        authors = authors.pivot_table(columns =\"method\")\n",
    "        authors.plot.bar(title=f\"Author Comparison Overview {name} with {preprocess_name}\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"bayes\", \"lda\"]:\n",
    "    method_df = df.loc[:, df.columns.str.contains(f\"(author|{method})\")]\n",
    "    for preprocessing in range(2,int((len(method_df.columns)))):\n",
    "        preprocess_name = re.sub(r\".*\\s(\\S+)$\", r\"\\1\", method_df.columns[preprocessing])\n",
    "        score_df = {}\n",
    "        for author in [\"EAP\", \"MWS\", \"HPL\"]:\n",
    "            author_df = pd.DataFrame(\n",
    "                {\"author\": method_df[\"author\"]})\n",
    "            author_df = pd.concat([author_df, method_df.iloc[:, 1:].apply(\n",
    "                lambda x: [1 if author == i else 0 for i in x])], axis=1)\n",
    "            author_df = author_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "            score_df[author] = [func(author_df[\"author_short\"],author_df.iloc[:, preprocessing]) for (name, func) in metrics]\n",
    "        score_df = pd.DataFrame(score_df)\n",
    "        score_df.index = [\"accuracy\", \"precision\", \"recall\", \"F1 score\"]\n",
    "        score_df.plot.bar(title=f\"Authors with {preprocess_name} using {method}\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"bayes\", \"lda\"]:\n",
    "    method_df = df.loc[:, df.columns.str.contains(f\"(?=.*author)|(?=.*{method})\")]\n",
    "    for preprocessing in range(2,int((len(method_df.columns)))):\n",
    "        preprocess_name = re.sub(r\".*\\s(\\S+)$\", r\"\\1\", method_df.columns[preprocessing])\n",
    "        display(method_df.groupby([\"author_short\", method_df.columns[preprocessing]]).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"bayes\", \"lda\"]:\n",
    "    for preprocessing in [\"WSPH\", \"CSPH\"]:\n",
    "        method_df = df.loc[:, df.columns.str.contains(f\"(?=.*author)|(?=.*{method})(?=.*{preprocessing})\")]\n",
    "        method_df = method_df.rename(columns=lambda x: re.sub(r\".*\\s(\\S+)$\", r\"\\1\", x))\n",
    "        score_df = method_df.iloc[: , 2:].apply(lambda x: [accuracy_score(x, method_df[\"author_short\"]), precision_score(method_df[\"author_short\"], x, average=\"macro\"), recall_score(method_df[\"author_short\"], x, average=\"macro\"), f1_score(method_df[\"author_short\"], x, average=\"macro\")])\n",
    "        score_df.index = [\"accuracy\", \"precision\", \"recall\", \"F1 score\"]\n",
    "        score_df = score_df.rename(columns=lambda x: re.sub(r'^\\D*(\\d.*)', r'\\1', x))\n",
    "        score_df.plot.bar(title=f\"Highpass Comparison with {preprocessing} using {method}\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, func) in metrics:\n",
    "    for method in [\"bayes\", \"lda\"]:\n",
    "        for preprocessing in [\"WSPH\", \"CSPH\"]:\n",
    "            method_df = df.loc[:, df.columns.str.contains(f\"(?=.*author)|(?=.*{method})(?=.*{preprocessing})\")]\n",
    "            cutoffs = defaultdict(lambda: [])\n",
    "            for author in [\"EAP\", \"HPL\", \"MWS\"]:\n",
    "                author_df = pd.DataFrame(\n",
    "                    {\"author\": method_df[\"author\"]})\n",
    "                author_df = pd.concat([author_df, method_df.iloc[:, 1:].apply(\n",
    "                    lambda x: [1 if author == i else 0 for i in x])], axis=1)\n",
    "                for cutoff in range(2,int(len(author_df.columns))):\n",
    "                    cutoffs[author_df.columns[cutoff]].append(func(author_df[\"author_short\"], author_df.iloc[:, cutoff]))\n",
    "            cutoffs = pd.DataFrame(cutoffs)\n",
    "            cutoffs.index = [authors_short_dict[author] for author in [\"EAP\", \"HPL\", \"MWS\"]]\n",
    "            cutoffs = cutoffs.rename(columns=lambda x: re.sub(r'^\\D*(\\d.*)', r'\\1', x))\n",
    "            cutoffs.plot.bar(title=f\"Author Highpass Comparison with {preprocessing} using {method}\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
